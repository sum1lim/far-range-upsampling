#!/usr/bin/env python3
import argparse
import torch
import numpy as np
import sklearn.metrics as metrics
from torch.utils.data import DataLoader
from far_range_upsampling.models import model_dict
from far_range_upsampling.utils import LidarData
from collections import OrderedDict


def main(args):
    print(f"Testing {args.params}")

    # Load Test Dataset
    lidar_data = LidarData("te", step_size=args.KNNstep)
    # Adjust batch size if the test dataset is too small
    if lidar_data.point.shape[0] < args.batch_size:
        args.batch_size = lidar_data.point.shape[0]
    # Test data loader
    te_data_loader = DataLoader(
        LidarData("te", step_size=args.KNNstep),
        num_workers=8,
        batch_size=args.batch_size,
        shuffle=True,
        drop_last=True,
    )

    # Load the model
    device = torch.device("cuda")
    model = model_dict[args.model](batch_size=args.batch_size, device=device)
    state_dict = torch.load(args.params, map_location=device)

    new_state_dict = OrderedDict()
    for k, v in state_dict.items():
        name = k[7:]
        new_state_dict[name] = v

    model.load_state_dict(new_state_dict)
    model.eval()
    model = model.double().to(device)

    count = 0.0
    model.eval()
    prediction = []
    ground_truth = []
    for point, data, label in te_data_loader:
        point, data, label = point.to(device), data.to(device), label.to(device)

        # Generate prediction
        logits = model(point.double(), data.double())
        preds = logits.flatten(start_dim=0, end_dim=1)
        # ground truth
        gt = label.flatten(start_dim=0, end_dim=1)

        count += args.batch_size
        ground_truth.append(gt.cpu().numpy())
        prediction.append(preds.detach().cpu().numpy())

    ground_truth = np.concatenate(ground_truth)
    prediction = np.concatenate(prediction)

    # Classes defined by thresholding of the distance
    prediction_class = prediction.copy()
    ground_truth_class = ground_truth.copy()
    prediction_class[prediction < 1000] = 1
    prediction_class[prediction >= 1000] = 0
    ground_truth_class[ground_truth < 1000] = 1
    ground_truth_class[ground_truth >= 1000] = 0

    # Metrics
    test_mae = metrics.mean_absolute_error(ground_truth, prediction)
    test_acc = metrics.balanced_accuracy_score(ground_truth_class, prediction_class)

    print(
        f"""
            Test MAE: {test_mae}
            Test accuracy: {test_acc}
        """
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--KNNstep",
        type=int,
        help="KNN step size",
    )
    parser.add_argument(
        "--model",
        type=str,
        choices=model_dict.keys(),
        help="Model to use for testing",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=4,
        help="Batch size",
    )
    parser.add_argument(
        "--params",
        type=str,
        help="Path to the saved parameters for the model",
    )

    args = parser.parse_args()

    main(args)
