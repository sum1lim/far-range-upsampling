#!/usr/bin/env python3
import argparse
import torch
import sys
import numpy as np
import time
import torch.optim as optim
import sklearn.metrics as metrics
from torch import nn
from far_range_upsampling.models import model_dict
from far_range_upsampling.utils import LidarData, loss_dict
from torch.utils.data import DataLoader


def main(args):
    log_file = open(f"./checkpoints/{args.exp_name}_train.log", "w")

    device = torch.device("cuda")
    BS = torch.cuda.device_count() * 8

    print(f"Number of GPUs: {torch.cuda.device_count()}", file=sys.stdout)
    print(f"Number of GPUs: {torch.cuda.device_count()}", file=log_file)
    print(f"Batch size: {BS}", file=sys.stdout)
    print(f"Batch size: {BS}", file=log_file)

    tr_data_loader = DataLoader(
        LidarData("tr", step_size=args.KNNstep),
        num_workers=8,
        batch_size=BS,
        shuffle=True,
        drop_last=True,
    )

    model = nn.DataParallel(model_dict[args.model](batch_size=BS, device=device)).to(
        device
    )

    epochs = 10000
    if args.loss == "focal":
        loss_func = loss_dict[args.loss](args.focal_thresh).to(device)
    elif args.loss == "combined":
        loss_func = loss_dict[args.loss](args.focal_thresh, args.focal_weight).to(
            device
        )
    else:
        loss_func = loss_dict[args.loss]().to(device)
    learning_rate = 0.001
    optim_net = optim.Adam(model.parameters(), lr=learning_rate)

    patience = 0
    patience_count = 0
    for epoch in range(epochs):
        best_loss = 0.0
        count = 0.0
        train_pred = []
        train_true = []
        idx = 0
        total_time = 0.0

        for point, data, label in tr_data_loader:
            point, data, label = point.to(device), data.to(device), label.to(device)

            start_time = time.time()

            logits = model(point, data)

            optim_net.zero_grad()
            loss = loss_func(logits, label)
            loss.backward(retain_graph=True)
            optim_net.step()
            end_time = time.time()
            total_time += end_time - start_time

            gt = label.flatten(start_dim=0, end_dim=1)
            preds = logits.flatten(start_dim=0, end_dim=1)
            count += BS
            train_loss += loss.item() * BS
            train_true.append(gt.cpu().numpy())
            train_pred.append(preds.detach().cpu().numpy())
            idx += 1

        print("train total time is", total_time, file=sys.stdout)
        print("train total time is", total_time, file=log_file)
        train_true = np.concatenate(train_true)
        train_pred = np.concatenate(train_pred)

        train_pred_class = np.absolute(train_pred)
        train_pred_class[train_pred_class < 1000] = 1
        train_pred_class[train_pred_class >= 1000] = 0
        train_true_class = np.absolute(train_true)
        train_true_class[train_true_class < 1000] = 1
        train_true_class[train_true_class >= 1000] = 0
        try:
            train_mae = metrics.mean_absolute_error(train_true, train_pred)
            train_acc = metrics.balanced_accuracy_score(
                train_true_class, train_pred_class
            )
        except ValueError:
            train_mae = "NaN"
            train_acc = "NaN"
        outstr = f"Epoch {epoch}, train MAE: {train_mae}, train accuracy: {train_acc} loss: {train_loss * 1.0 / count}"
        print(outstr, file=sys.stdout)
        print(outstr, file=log_file)

        if train_loss < best_loss:
            patience = 0
            best_loss = train_loss
            torch.save(model.state_dict(), f"checkpoints/{args.exp_name}_model.pt")

        patience += 1
        if patience > args.patience:
            if patience_count < 3:
                patience = 0
                patience_count += 1
                learning_rate /= 10
                print("learning rate change", file=sys.stdout)
                print("learning rate change", file=log_file)
                optim_net = optim.Adam(model.parameters(), lr=learning_rate)
            else:
                print("early stopping", file=sys.stdout)
                print("early stopping", file=log_file)
                break


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--exp-name",
        type=str,
        help="Name of the experiment",
    )
    parser.add_argument(
        "--KNNstep",
        type=int,
        default=1,
        help="KNN step size",
    )
    parser.add_argument(
        "--patience",
        type=int,
        default=50,
        help="Patience",
    )
    parser.add_argument(
        "--model",
        type=str,
        choices=model_dict.keys(),
        help="Model to use for training",
    )
    parser.add_argument(
        "--loss",
        type=str,
        choices=loss_dict.keys(),
        help="Loss function",
    )
    parser.add_argument(
        "--focal-thresh",
        type=int,
        default=2000,
        help="Distance threhold required for focal loss for classification",
    )
    parser.add_argument(
        "--focal-weight",
        type=float,
        default=1,
        help="Weight of the focal loss in the combined loss",
    )

    args = parser.parse_args()

    main(args)
